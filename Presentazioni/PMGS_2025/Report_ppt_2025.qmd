---
title: "Project Advancement 2024/2025"
format: 
  revealjs:
    theme: simple
    center: true
    slide-number: true
    toc: true
    toc-title: "Content"
author: Ambra Perugini 
bibliography: bibmeta.bib
csl: apa.csl
self-contained: true
self-contained-math: true
embed-resources: true
---

```{r, echo=FALSE,warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE,prompt=TRUE,comment=NA,warning=FALSE,message=FALSE,results="as.is",cache=TRUE)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
rm(list=ls()) # Clear the workspace

## Loading packages for analyses
library(readxl) # to read data
library(metafor) # maximum likelihood meta-analysis
library(bayesmeta) # bayesian meta-analysis
library(criticalESvalue)

## Import the data "dataset_meta.xlsx" in R
d <- as.data.frame( read_excel("/Users/ambraperugini/Documents/Work/Track of work/2024_2025/Report_presentazione_2025/dataset_meta.xlsx") )
d$TYPE_OF_STIMULI <- factor(d$TYPE_OF_STIMULI)

# recode "type of stimuli" (see Pairwise comparison below)
d$TYPE_OF_STIMULIc <- relevel(d$TYPE_OF_STIMULI,"VISUOSPATIAL") 

## Building 3 datasets, one for each memory system
data_long <- d[d$MEMORY_SYSTEM=="LONG TERM",]
data_short<-d[d$MEMORY_SYSTEM=="SHORT TERM",]
data_wm<-d[d$MEMORY_SYSTEM=="WM",]

## PET-PEESE analysis for publication bias correction

# Add standard error to each subset
data_long$SE <- sqrt(data_long$Variance)
data_short$SE <- sqrt(data_short$Variance)
data_wm$SE <- sqrt(data_wm$Variance)

```


# Where were we?

The project: 

- Enhancing replicability of Psychological Science Research Field: Critical effect size values and the Overlapping Index. 

What was done:

- Permutation test applied to the Overlapping Index

- The benefits of reporting critical effect size values


## What now and Why? {.unlisted}

- I am trying to apply critical effect size values to meta-analysis.  The goal is to demonstrate the utility of critical effect size values in practice and also evaluate with some examples what is going on in the field of psychological sciences.

- Through simulation I am confronting the Overlapping Index with the Cohen's d evaluating differences in bias.

# Is my meta-analysis useless? Assessment of cumulative research through critical effect size values

## Recap of Critical Effect Size Values {.unlisted}

The smallest detectable effect that can reach statistical significance given a specific sample size, alpha level, and test statistic.

Basically converting the critical t into a critical Cohen's d for example.

The bigger the sample size, the smaller the value that can reach statistical significance. 

```{r PET}
# --- LONG TERM MEMORY ---
# PET (effect ~ SE)
pet_long <- rma(yi = Hedges_g, sei = SE, mods = ~ SE, data = data_long, method = "REML")


# PEESE (effect ~ SE^2)
peese_long <- rma(yi = Hedges_g, sei = SE, mods = ~ I(SE^2), data = data_long, method = "REML")


# --- SHORT TERM MEMORY ---
pet_short <- rma(yi = Hedges_g, sei = SE, mods = ~ SE, data = data_short, method = "REML")


peese_short <- rma(yi = Hedges_g, sei = SE, mods = ~ I(SE^2), data = data_short, method = "REML")


# --- WORKING MEMORY ---
pet_wm <- rma(yi = Hedges_g, sei = SE, mods = ~ SE, data = data_wm, method = "REML")


peese_wm <- rma(yi = Hedges_g, sei = SE, mods = ~ I(SE^2), data = data_wm, method = "REML")


```


## Case studies {.unlisted}


We want to have different scenarios:

- a meta-analysis with typical sample sizes in psychology

- one with large sample size for individual studies to show also an example of big sample sizes and what it means


## @talamini2017musicians {.smaller}


We have decided to use the meta-analysis by @talamini2017musicians as an example of small samples. 

It calculates meta-analytic effects for musicians and non-musicians on:

- long term memory,

- short term memory  

- working memory 

We will compute critical values for two tailed t-test, assuming alpha at the .05 threshold and plot the actual effect of the individual study and the critical value for each one. 

```{r, warning=FALSE, message=FALSE}
Meta_g_L <- .29 # meta-effect
MetaB_g_L <- .21 # meta-effect corrected for bias


data_long$Critical_2t <- rep(NA,nrow(data_long))
data_long$Critical_1t <- rep(NA,nrow(data_long))
c1 <- match("N_MUSICIANS",names(data_long))
c2 <- match("N_NONMUSICIANS",names(data_long))
Q <- 0

for(i in 1:nrow(data_long)){
  
  n1 <- data_long[i,c1]
  n2 <- data_long[i,c2]

  gc_2t <- critical_t2s(n1 = n1, n2 = n2, hypothesis = "two.sided", conf.level = 0.95)$gc

  gc_1t <- critical_t2s(n1 = n1, n2 = n2, hypothesis = "greater", conf.level = 0.95)$gc
  
  Q <- Q + 1
  data_long[Q, "Critical_2t"] <- gc_2t
  data_long[Q, "Critical_1t"] <- gc_1t
}

P_l <- sum(data_long$Critical_2t < Meta_g_L)/length(data_long$Critical_2t)*100
PB_l <- sum(data_long$Critical_2t < MetaB_g_L)/length(data_long$Critical_2t)*100

```


```{r, warning=FALSE, message=FALSE}
Meta_g_S <- .57 # meta-effect
MetaB_g_S <- .39 # meta-effect corrected for bias

data_short$Critical_2t <- rep(NA,nrow(data_short))
data_short$Critical_1t <- rep(NA,nrow(data_short))
c1 <- match("N_MUSICIANS",names(data_short))
c2 <- match("N_NONMUSICIANS",names(data_short))
Q <- 0

for(i in 1:nrow(data_short)){
  
  n1 <- data_short[i,c1]
  n2 <- data_short[i,c2]

  gc_2t <- critical_t2s(n1 = n1, n2 = n2, hypothesis = "two.sided", conf.level = 0.95)$gc

  gc_1t <- critical_t2s(n1 = n1, n2 = n2, hypothesis = "greater", conf.level = 0.95)$gc
  
  Q <- Q + 1
  data_short[Q, "Critical_2t"] <- gc_2t
  data_short[Q, "Critical_1t"] <- gc_1t
}

P_s <- sum(data_short$Critical_2t < Meta_g_S)/length(data_short$Critical_2t)*100
PB_s<- sum(data_short$Critical_2t < MetaB_g_S)/length(data_short$Critical_2t)*100

```

```{r, warning=FALSE, message=FALSE}
Meta_g_WM <- .56 # meta-effect


data_wm$Critical_2t <- rep(NA,nrow(data_wm))
data_wm$Critical_1t <- rep(NA,nrow(data_wm))
c1 <- match("N_MUSICIANS",names(data_wm))
c2 <- match("N_NONMUSICIANS",names(data_wm))
Q <- 0

for(i in 1:nrow(data_wm)){
  
  n1 <- data_wm[i,c1]
  n2 <- data_wm[i,c2]

  gc_2t <- critical_t2s(n1 = n1, n2 = n2, hypothesis = "two.sided", conf.level = 0.95)$gc

  gc_1t <- critical_t2s(n1 = n1, n2 = n2, hypothesis = "greater", conf.level = 0.95)$gc
  
  Q <- Q + 1
  data_wm[Q, "Critical_2t"] <- gc_2t
  data_wm[Q, "Critical_1t"] <- gc_1t
}

P_wm <- sum(data_wm$Critical_2t < Meta_g_WM)/length(data_wm$Critical_2t)*100


```

## Long Term Memory {.unlisted}

```{r, warning=FALSE, message=FALSE}
library(tidyverse)

# Create a data frame for the vertical lines
vline_data <- data.frame(
  LineType = factor(
    c("Meta-analytic effect", "Bias-corrected effect"),
    levels = c("Hedges_g", "Critical_2t", "Meta-analytic effect", "Bias-corrected effect")
  ),
  x = c(Meta_g_L, MetaB_g_L)
)

data_long %>%
  select(AUTHORS, Hedges_g, Critical_2t) %>%
  pivot_longer(cols = c(Hedges_g, Critical_2t), names_to = "Measure", values_to = "Value") %>%
  mutate(Measure = factor(Measure, levels = c("Hedges_g", "Critical_2t", "Meta-analytic effect", "Bias-corrected effect"))) %>%
  ggplot(aes(x = Value, y = reorder(AUTHORS, Value), color = Measure)) +
  geom_point(size = 3) +
  geom_vline(data = vline_data, aes(xintercept = x, color = LineType),
             linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  scale_color_manual(
    values = c(
      "Hedges_g" = "steelblue",
      "Critical_2t" = "firebrick",
      "Meta-analytic effect" = "black",
      "Bias-corrected effect" = "grey"
    )
  ) +
  labs(
    title = "Effect Sizes vs Critical Values",
    x = "Value",
    y = "Study (Author)",
    color = "Measure / Reference Line"
  ) +
  theme_minimal()


```

## Long Term Memory {.unlisted}

We compared the estimated meta-analytic effect size to the critical values of individual studies. In `r P_l`\% of cases, the critical value was lower than the meta-analytic effect size, indicating that these studies would have been able to detect a statistically significant effect of that magnitude. When comparing the critical values to the bias corrected meta-analytic effect size the percentage was still `r PB_l`\%, as it couldn't go any lower.

## Short Term Memory {.unlisted}

```{r, warning=FALSE, message=FALSE}


vline_data <- data.frame(
  LineType = factor(
    c("Meta-analytic effect", "Bias-corrected effect"),
    levels = c("Hedges_g", "Critical_2t", "Meta-analytic effect", "Bias-corrected effect")
  ),
  x = c(Meta_g_S, MetaB_g_S)
)

data_short %>%
  select(AUTHORS, Hedges_g, Critical_2t) %>%
  pivot_longer(cols = c(Hedges_g, Critical_2t), names_to = "Measure", values_to = "Value") %>%
  mutate(Measure = factor(Measure, levels = c("Hedges_g", "Critical_2t", "Meta-analytic effect", "Bias-corrected effect"))) %>%
  ggplot(aes(x = Value, y = reorder(AUTHORS, Value), color = Measure)) +
  geom_point(size = 3) +
  geom_vline(data = vline_data, aes(xintercept = x, color = LineType),
             linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  scale_color_manual(
    values = c(
      "Hedges_g" = "steelblue",
      "Critical_2t" = "firebrick",
      "Meta-analytic effect" = "black",
      "Bias-corrected effect" = "grey"
    )
  ) +
  labs(
    title = "Effect Sizes vs Critical Values",
    x = "Value",
    y = "Study (Author)",
    color = "Measure / Reference Line"
  ) +
  theme_minimal()

```

## Short Term Memory {.unlisted}

In `r P_s`\% of cases, the critical value was lower than the meta-analytic effect size, indicating that these studies would have been able to detect a statistically significant effect of that magnitude. When comparing the critical values to the bias corrected meta-analytic effect size the percentage lowered to `r PB_s`\%.

## Working Memory {.unlisted}

```{r, warning=FALSE, message=FALSE}

vline_data <- data.frame(
  LineType = factor("Meta-analytic effect",
                    levels = c("Hedges_g", "Critical_2t", "Meta-analytic effect")),
  x = Meta_g_WM
)

data_wm %>%
  select(AUTHORS, Hedges_g, Critical_2t) %>%
  pivot_longer(cols = c(Hedges_g, Critical_2t), names_to = "Measure", values_to = "Value") %>%
  mutate(Measure = factor(Measure, levels = c("Hedges_g", "Critical_2t", "Meta-analytic effect"))) %>%
  ggplot(aes(x = Value, y = reorder(AUTHORS, Value), color = Measure)) +
  geom_point(size = 3) +
  geom_vline(data = vline_data, aes(xintercept = x, color = LineType),
             linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  scale_color_manual(
    values = c(
      "Hedges_g" = "steelblue",
      "Critical_2t" = "firebrick",
      "Meta-analytic effect" = "black"
    )
  ) +
  labs(
    title = "Effect Sizes vs Critical Values",
    x = "Value",
    y = "Study (Author)",
    color = "Measure / Reference Line"
  ) +
  theme_minimal()

```

## Working Memory {.unlisted}

We compared the estimated meta-analytic effect size to the critical values of individual studies. In `r round(P_wm,2)`\% of cases, the critical value was lower than the meta-analytic effect size, indicating that these studies would have been able to detect a statistically significant effect of that magnitude.

## What have subsequent studies done? {.unlisted .smaller}

```{r, eval=FALSE}
cited_meta <- read.csv("~/Documents/Work/projects/Critical-x-Meta/cited_meta.csv", sep=";")
#colnames(cited_meta)
```

Of the 215 citations, pdf's were available for 133 papers. Then only 36 reported a power analysis.

|  | Count | Denominator | Percent |
|-------------|-----|------|------|
| Citations with available PDFs | 133 | 215 | `r round(133/215*100,2)`\% |
| Studies reporting any power analysis (among those with PDFs) | 36 | 133 | `r round(36/133*100,2)`\% |
| Power analysis informed by effects of @talamini2017musicians | 0 | 36 | `r round(0/36*100,2)`\% |
| Reported which analysis the power referred to | 13 | 36 | `r round(13/36*100,2)`\% |
| α level stated | 20 | 36 | `r round(20/36*100,2)`\% |

## What have subsequent studies done? {.unlisted .smaller}

|  | Count | Denominator | Percent |
|-------------|-----|------|------|
| Effect size value for desired power reported | 24 | 36 | `r round(24/36*100,2)`\% |
| Type of effect size (for which power was computed) reported | 23 | 36 | `r round(23/36*100,2)`\% |
| Desired power level reported | 28 | 36 | `r round(28/36*100,2)`\% |
| Desired sample size reported | 30 | 36 | `r round(30/36*100,2)`\% |
| All information needed to reproduce the power analysis reported | 13 | 36 | `r round(13/36*100,2)`\% |

## @shannon2022problematic

The meta-analysis by @shannon2022problematic has big sample sizes for individual studies and a meta-analytic significant effect for the correlation between depression and social media use in young adolescents.

```{r}

rm(list=ls())

library(criticalESvalue)
library(ggplot2)
library(tidyverse)
library(tibble)

data_bm <- tibble(
  Author_Year = c("Holmgren (2017)", "Wang (2018)", "Apaolaza (2019)", "Hou (2019)", 
                  "Kircaburun (2019)", "Mitra (2019)", "Chen (2020)", "Kim (2020)",
                  "Kircaburun, Demetrovics (2020)", "Kircaburun, Grifiths (2020)",
                  "Stockdale (2020)", "Wong (2020)", "Yildiz (2020)",
                  "Brailovskaia; Lithuanian sample (2021)", "Brailovskaia; German sample (2021)",
                  "Giordano (2021)", "He (2021)", "Kilincel (2021)"),
  n = c(442, 365, 346, 641, 470, 264, 437, 209, 344, 460, 385, 300, 451, 1640, 727, 428, 218, 1142),
  Female_n_pct = c("228 (51.6)", "190 (52)", "179 (51.7)", "477 (74.4)", "280 (59.6)", "164 (62.2)",
                   "308 (70.5)", "31 (14.8)", "282 (82)", "281 (61)", "204 (53)", "178 (59.3)",
                   "214 (47.5)", "1123 (68.5)", "548 (75.4)", "218 (50.9)", "218 (100)", "722 (63.2)"),
  Male_n_pct = c("214 (48.4)", "175 (48)", "167 (48.3)", "164 (25.6)", "190 (40.4)", "100 (37.8)",
                 "129 (29.5)", "178 (85.2)", "62 (18)", "179 (39)", "181 (47)", "122 (40.7)",
                 "237 (52.5)", "517 (31.5)", "179 (24.6)", "210 (49.1)", "0 (0)", "420 (36.8)"),
  Age_Range_Mean = c("18-21 (18.86)", "14-18 (16.29)", "17-26 (18.73)", "17-25 (19.9)",
                     "14-18 (16.29)", "18-25 (21.56)", "16-30 (24.21)", "15-18 (N/A)",
                     "18-25 (20.87)", "18-26 (19.74)", "17-19 (18.01)", "18-24 (20.89)",
                     "13-17 (15.5)", "18-29 (19.09)", "18-29 (21.47)", "13-19 (17.38)",
                     "19-23 (19.6)", "12-18 (15.6)"),
  Country = c("United States", "China", "Spain", "China", "Turkey", "India", "China", "China",
              "Turkey", "Turkey", "United States", "Hong Kong", "Turkey",
              "Lithuania", "Germany", "United States", "China", "Turkey"),
  Depress_r = c(0.29, 0.18, NA, 0.22, 0.03, 0.39, NA, NA, 0.22, 0.34, 0.28, 0.336, NA, 0.305, 0.396, NA, NA, NA),
  Anxiety_r = c(NA, NA, NA, 0.22, NA, NA, 0.29, 0.20, NA, NA, 0.24, 0.344, 0.58, 0.329, 0.461, NA, NA, 0.417),
  Stress_r = c(NA, NA, 0.49, 0.11, NA, NA, NA, NA, NA, NA, NA, 0.384, NA, 0.246, 0.411, NA, 0.23, NA),
  Combined_r = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.314, NA, NA)
)

data_d_r <- subset(data_bm, !is.na(Depress_r))

meta_r_d <- 0.273

data_d_r$Critical_r <- rep(NA,nrow(data_d_r))
c <- match("n",names(data_d_r))
Q <- 0

for(i in 1:nrow(data_d_r)){
  
  n <- data_d_r[[i,c]]

  c_r <- critical_cor(n = n, hypothesis = "two.sided", conf.level = 0.95)$rc

  Q <- Q + 1
  data_d_r[Q, "Critical_r"] <- c_r
}

P_d_r <- sum(data_d_r$Critical_r < meta_r_d)/length(data_d_r$Critical_r)*100



```

##  @shannon2022problematic {.unlisted}

```{r}
vline_data <- data.frame(
  LineType = factor("Meta-analytic effect",
                    levels = c("Depress_r", "Critical_r", "Meta-analytic effect")),
  x = meta_r_d
)

data_d_r %>%
  select(Author_Year, Depress_r, Critical_r) %>%
  pivot_longer(cols = c(Depress_r, Critical_r), names_to = "Measure", values_to = "Value") %>%
  mutate(Measure = factor(Measure, levels = c("Depress_r", "Critical_r", "Meta-analytic effect"))) %>%
  ggplot(aes(x = Value, y = reorder(Author_Year, Value), color = Measure)) +
  geom_point(size = 3) +
  geom_vline(data = vline_data, aes(xintercept = x, color = LineType),
             linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  scale_color_manual(
    values = c(
      "Depress_r" = "steelblue",
      "Critical_r" = "firebrick",
      "Meta-analytic effect" = "black"
    )
  ) +
  labs(
    title = "Effect Sizes vs Critical Values",
    x = "Value",
    y = "Study",
    color = "Measure / Reference Line"
  ) +
  theme_minimal()

```

##  @shannon2022problematic {.unlisted}

This case is a good example of all studies having critical effect size values lower than the meta-analytic effect (`r P_d_r`\%) meaning that even the individual studies where suited to find a significant effect if there was one as in this case, it is also probably due to the nature of the research field of online data collection that makes it easier to have bigger samples.

## Conclusions
 
Even though some fields have big sample sizes, this is not always the case in psychology. In the case of the meta-analysis on musicians, previous research is not carried out to inform future research planning, even when research design is compatible with information from previous meta-analysis.

-> To be able to give a broader overview of the psychological sciences we would like to do the same **on more meta-analysis**.

<!-- One thing is sure: **there is still a long way to go before power analysis and design analysis are carried out with awareness and proper reasoning** -->

# Overlapping Index and Cohen's d

## Overview {.unlisted}

The overlapping index calculates the area of overlap of two density distributions, it is 0 when there is no overlap and 1 when there is complete overlap.

The Cohen's d calculates standardized mean difference between two groups or conditions.

## Simulation design

::: {style="font-size: 95%;"}
-  n = ***10, 50, 100, 300, 500***, equal in the two samples.

-  Mean difference: $\delta$ = ***0, 1***; 

-  Variability: $\sigma$ = ***1, 5***;

-  Skewness: $\alpha$ =***0, 5***;
:::

For the total combination of $5 × 2 × 2 × 2 = 40$ conditions we generated $500$ sets of data on which we performed the analysis.

## {.unlisted}

![](img/skewnorm.png)

## Scenarios {.unlisted}

![](img/scenarios.png)

## Results 

We compare the Overlapping Index and the Cohen's d through Means Square Error (MSE) to evaluate both bias in the estimates and variability.

The MSE is defined as:

$\sigma_j^2 + [1/B \sum_b  (\hat{\theta}_{bj}-\theta_j)]^2$

with $\sigma_j^2$ being the variance of the estimates across replications and $\theta_j$ is the true value in the *j* condition and $\hat{\theta}_{bj}$ is the estimated one in the *b* replicate. The lower the MSE, the closer is forecast to actual.

## Mean Square Error $\delta = 0$ {.unlisted .smaller}

:::: {.columns}

::: {.column width="80%"}

![](img/Delta_0.png)

:::

::: {.column width="20%"}

Row panels represent $\alpha$ values (kurtosis) and column panels the values of $\omega$ (skewness)

:::

::::

## Mean Square Error $\delta = 1$ {.unlisted .smaller}

:::: {.columns}

::: {.column width="80%"}

![](img/Delta_1.png)

:::

::: {.column width="20%"}

Row panels represent $\alpha$ values (kurtosis) and column panels the values of $\omega$ (skewness)

:::

::::

## Conclusion {.unlisted}

Especially with small sample sizes, the Overlapping Index shows less variability in the estimates compared to the Cohen's *d* and is a more precise estimator. Even though the two indexes differ in what they detect, they are in some cases interchangeable and we advice to use the Overlapping Index in those cases where assumptions are not respected.

# Next steps 

- For my PhD project this will be it but I would like to expand my knowledge and interest in the field of meta-science and simulation studies skills such as power analysis and performance estimation.

For now:

- Wrap up those last two projects

- Write PhD Thesis


## References {.unlisted}

